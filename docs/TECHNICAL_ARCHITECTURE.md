# NutriWallet â€” Technical Architecture

## Overview

This document outlines the technical architecture for NutriWallet, a budget-first meal planning application that optimizes nutrition within a user's budget constraint.

**Core Flow:** `Budget â†’ Optimize Nutrition â†’ Source Ingredients`

---

## Validated Markets

| Market | Store | Data Quality | Status |
|--------|-------|--------------|--------|
| ğŸ‡¦ğŸ‡¹ Austria | SPAR, Billa, Hofer | âœ… Excellent | Validated |
| ğŸ‡ºğŸ‡¸ USA | Walmart | âœ… Good | Validated |
| ğŸ‡¬ğŸ‡§ UK | Tesco | âœ… Excellent | Validated |
| ğŸ‡®ğŸ‡³ India | BigBasket | âœ… Excellent | Validated |

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER INTERFACE                               â”‚
â”‚                    (Next.js / React Frontend)                          â”‚
â”‚         Budget Input â†’ City Selection â†’ Dietary Preferences            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ORCHESTRATOR AGENT                             â”‚
â”‚                           (OpenAI API)                                 â”‚
â”‚                                                                        â”‚
â”‚   â€¢ Receives user request (budget, city, preferences)                 â”‚
â”‚   â€¢ Coordinates sub-agents                                            â”‚
â”‚   â€¢ Assembles final meal plan                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
            â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RESEARCHER AGENT    â”‚         â”‚          OPTIMIZER AGENT             â”‚
â”‚                       â”‚         â”‚                                      â”‚
â”‚ PRIMARY: Agentic      â”‚         â”‚ â€¢ Linear programming                 â”‚
â”‚   Web Search          â”‚         â”‚ â€¢ Maximize: protein/nutrition        â”‚
â”‚                       â”‚ â”€â”€â”€â”€â”€â”€â–¶ â”‚ â€¢ Subject to: budget constraint      â”‚
â”‚ FALLBACK: Price       â”‚ prices  â”‚ â€¢ Constraints: variety, meal types   â”‚
â”‚   Repository (DB)     â”‚         â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                         â”‚
            â–¼                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PRICE REPOSITORY    â”‚                 â”‚    REPORTER AGENT     â”‚
â”‚      (Postgres)       â”‚                 â”‚                       â”‚
â”‚                       â”‚                 â”‚ â€¢ Generates meal plan â”‚
â”‚ â€¢ Cached prices       â”‚                 â”‚ â€¢ Shopping list       â”‚
â”‚ â€¢ Updated via         â”‚                 â”‚ â€¢ Store links         â”‚
â”‚   scheduled scraping  â”‚                 â”‚ â€¢ Nutritional summary â”‚
â”‚ â€¢ 7-day freshness     â”‚                 â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Layer: Dual-Source Strategy

### Why Dual-Source?

1. **Agentic Search (Primary):** Real-time, always fresh, but can fail
2. **Price Repository (Fallback):** Reliable, fast, but may be slightly stale

Grocery prices don't change daily â€” weekly updates are sufficient for MVP.

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA ACQUISITION LAYER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  REAL-TIME PATH     â”‚      â”‚      SCHEDULED SCRAPING PATH        â”‚ â”‚
â”‚   â”‚  (On User Request)  â”‚      â”‚      (Background Job)               â”‚ â”‚
â”‚   â”‚                     â”‚      â”‚                                     â”‚ â”‚
â”‚   â”‚  User Request       â”‚      â”‚  Cron Job (Weekly)                  â”‚ â”‚
â”‚   â”‚       â”‚             â”‚      â”‚       â”‚                             â”‚ â”‚
â”‚   â”‚       â–¼             â”‚      â”‚       â–¼                             â”‚ â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚   â”‚  â”‚ Jina Reader â”‚    â”‚      â”‚  â”‚      Crawl4AI               â”‚   â”‚ â”‚
â”‚   â”‚  â”‚ (Fast,Free) â”‚    â”‚      â”‚  â”‚  (Full scraper, async)      â”‚   â”‚ â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚   â”‚         â”‚           â”‚      â”‚                 â”‚                  â”‚ â”‚
â”‚   â”‚         â–¼           â”‚      â”‚                 â–¼                  â”‚ â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚   â”‚  â”‚ LLM Parse   â”‚    â”‚      â”‚  â”‚   Structured Extraction     â”‚   â”‚ â”‚
â”‚   â”‚  â”‚ (OpenAI API)â”‚    â”‚      â”‚  â”‚   (CSS + LLM Hybrid)        â”‚   â”‚ â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚   â”‚         â”‚           â”‚      â”‚                 â”‚                  â”‚ â”‚
â”‚   â”‚         â–¼           â”‚      â”‚                 â–¼                  â”‚ â”‚
â”‚   â”‚  Return to User     â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚   â”‚  + Cache in DB      â”‚      â”‚  â”‚   Price Repository (DB)     â”‚   â”‚ â”‚
â”‚   â”‚                     â”‚      â”‚  â”‚   â€¢ product_name            â”‚   â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚   â€¢ price                   â”‚   â”‚ â”‚
â”‚                                â”‚  â”‚   â€¢ unit (kg, L, piece)     â”‚   â”‚ â”‚
â”‚                                â”‚  â”‚   â€¢ store                   â”‚   â”‚ â”‚
â”‚                                â”‚  â”‚   â€¢ city                    â”‚   â”‚ â”‚
â”‚                                â”‚  â”‚   â€¢ scraped_at              â”‚   â”‚ â”‚
â”‚                                â”‚  â”‚   â€¢ url                     â”‚   â”‚ â”‚
â”‚                                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Web Scraping Stack

### Primary Tool: Crawl4AI (FREE, Open Source)

**Why Crawl4AI?**
- 56k+ GitHub stars (#1 trending)
- 100% free, no API keys needed
- Async browser pool (fast)
- LLM-ready markdown output
- Supports scheduled scraping
- Self-hosted = full control

**Installation:**
```bash
pip install -U crawl4ai
crawl4ai-setup  # Installs Playwright browsers
```

**Basic Usage:**
```python
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def scrape_prices(url: str):
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url=url,
            config=CrawlerRunConfig(
                word_count_threshold=1,
                page_timeout=30000,
            )
        )
        return result.markdown  # Clean, LLM-ready output
```

### Fallback Tool: Jina Reader (FREE, Simple)

For quick, single-page extractions:
```python
import requests

def quick_scrape(url: str) -> str:
    """Prepend r.jina.ai/ to any URL for instant markdown"""
    jina_url = f"https://r.jina.ai/{url}"
    response = requests.get(jina_url)
    return response.text
```

### Tool Comparison

| Feature | Crawl4AI | Jina Reader | Firecrawl |
|---------|----------|-------------|-----------|
| Cost | FREE | FREE | $16+/mo |
| Self-hosted | âœ… | âŒ | Partial |
| Async/Batch | âœ… | âŒ | âœ… |
| JS Rendering | âœ… | âœ… | âœ… |
| Scheduled Jobs | âœ… | âŒ | âœ… |
| Best For | Bulk scraping | Quick lookups | Cloud API |

---

## Database Schema

### PostgreSQL (via Supabase)

```sql
-- Products table (scraped items)
CREATE TABLE products (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name TEXT NOT NULL,
    normalized_name TEXT NOT NULL,  -- "chicken breast" not "SPAR HÃ¼hnerbrust"
    category TEXT NOT NULL,         -- protein, carbs, vegetables, dairy, etc.
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Prices table (price observations)
CREATE TABLE prices (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    product_id UUID REFERENCES products(id),
    store TEXT NOT NULL,            -- spar, billa, walmart, tesco, bigbasket
    city TEXT NOT NULL,             -- vienna, london, mumbai, etc.
    country TEXT NOT NULL,          -- AT, US, UK, IN
    price DECIMAL(10,2) NOT NULL,
    currency TEXT NOT NULL,         -- EUR, USD, GBP, INR
    unit TEXT NOT NULL,             -- kg, L, piece, 100g
    price_per_unit DECIMAL(10,2),   -- normalized price per kg/L
    original_url TEXT,
    scraped_at TIMESTAMPTZ DEFAULT NOW(),
    is_on_sale BOOLEAN DEFAULT FALSE,
    sale_price DECIMAL(10,2)
);

-- Nutritional data (static, seeded once)
CREATE TABLE nutrition (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    product_id UUID REFERENCES products(id),
    calories_per_100g INTEGER,
    protein_per_100g DECIMAL(5,2),
    carbs_per_100g DECIMAL(5,2),
    fat_per_100g DECIMAL(5,2),
    fiber_per_100g DECIMAL(5,2)
);

-- Scrape jobs (tracking)
CREATE TABLE scrape_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    store TEXT NOT NULL,
    city TEXT NOT NULL,
    status TEXT DEFAULT 'pending',  -- pending, running, completed, failed
    items_scraped INTEGER DEFAULT 0,
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    error_message TEXT
);

-- Indexes for fast lookups
CREATE INDEX idx_prices_store_city ON prices(store, city);
CREATE INDEX idx_prices_product_scraped ON prices(product_id, scraped_at DESC);
CREATE INDEX idx_products_category ON products(category);
```

---

## Scheduled Scraping Strategy

### Scrape Frequency

| Store Type | Frequency | Reason |
|------------|-----------|--------|
| Supermarkets | Weekly | Prices change with weekly flyers |
| Discount stores | 2x/week | Flash sales more common |
| Online-only | Daily | More dynamic pricing |

### Cron Schedule (Example)

```bash
# Weekly scrape - Sunday 2 AM (before new week's prices)
0 2 * * 0 /path/to/scrape_all_stores.py

# Mid-week update for discount stores
0 2 * * 3 /path/to/scrape_discount_stores.py
```

### Scraping Priority List (MVP)

**Phase 1 (Week 1):**
- Austria: SPAR (spar.at)
- UK: Tesco (tesco.com)

**Phase 2 (Week 2):**
- India: BigBasket (bigbasket.com)
- Austria: Billa, Hofer

**Phase 3 (Week 3):**
- USA: Walmart (walmart.com)
- UK: Sainsbury's, Asda

### Products to Scrape (MVP - High Protein Focus)

```python
MVP_PRODUCTS = {
    "protein": [
        "chicken breast",
        "eggs",
        "greek yogurt",
        "cottage cheese",
        "tofu",
        "lentils",
        "chickpeas",
        "tuna",
        "ground beef",
        "milk",
    ],
    "carbs": [
        "rice",
        "oats",
        "bread",
        "pasta",
        "potatoes",
        "bananas",
    ],
    "vegetables": [
        "broccoli",
        "spinach",
        "carrots",
        "onions",
        "tomatoes",
    ],
    "fats": [
        "olive oil",
        "peanut butter",
        "butter",
    ],
}
```

---

## Tech Stack Summary

### Backend
| Component | Technology | Why |
|-----------|------------|-----|
| Language | Python 3.11+ | Best for AI/ML, scraping |
| Web Framework | FastAPI | Async, fast, modern |
| Scraping | Crawl4AI | Free, powerful, LLM-ready |
| Quick Scrape | Jina Reader | Simple fallback |
| Database | PostgreSQL (Supabase) | Reliable, free tier |
| Task Queue | Redis + Celery | Scheduled scraping |
| LLM | OpenAI API | Primary model provider (framework TBD, e.g. OpenAI Agents / PydanticAI) |

### Frontend
| Component | Technology | Why |
|-----------|------------|-----|
| Framework | Next.js 14+ | SSR, fast, familiar |
| Styling | Tailwind CSS | Rapid UI development |
| State | Zustand | Simple, lightweight |
| Forms | React Hook Form | Easy validation |

### Infrastructure
| Component | Technology | Why |
|-----------|------------|-----|
| Hosting (Backend) | Railway / Render | Easy Python deployment |
| Hosting (Frontend) | Vercel | Next.js native |
| Database | Supabase | Free tier, Postgres |
| Cron Jobs | Railway / GitHub Actions | Free scheduled tasks |

---

## API Endpoints (MVP)

```
POST /api/meal-plan
  Body: { budget: 50, currency: "EUR", city: "vienna", preferences: {...} }
  Returns: { meals: [...], shopping_list: [...], total_cost: 48.50 }

GET /api/prices?city=vienna&category=protein
  Returns: [{ product: "chicken breast", price: 8.99, store: "spar", ... }]

GET /api/stores?city=vienna
  Returns: [{ name: "SPAR", url: "spar.at" }, ...]

POST /api/scrape/trigger  (admin only)
  Body: { store: "spar", city: "vienna" }
  Returns: { job_id: "...", status: "started" }
```

---

## Project Structure

```
NutriWallet/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ MVP.md
â”‚   â””â”€â”€ TECHNICAL_ARCHITECTURE.md
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”‚   â”œâ”€â”€ config.py            # Environment config
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”œâ”€â”€ orchestrator.py  # Main coordinator
â”‚   â”‚   â”‚   â”œâ”€â”€ researcher.py    # Price fetching
â”‚   â”‚   â”‚   â”œâ”€â”€ optimizer.py     # Meal optimization
â”‚   â”‚   â”‚   â””â”€â”€ reporter.py      # Output formatting
â”‚   â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py          # Base scraper class
â”‚   â”‚   â”‚   â”œâ”€â”€ crawl4ai_scraper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ jina_scraper.py
â”‚   â”‚   â”‚   â””â”€â”€ stores/
â”‚   â”‚   â”‚       â”œâ”€â”€ spar.py
â”‚   â”‚   â”‚       â”œâ”€â”€ tesco.py
â”‚   â”‚   â”‚       â”œâ”€â”€ bigbasket.py
â”‚   â”‚   â”‚       â””â”€â”€ walmart.py
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py        # SQLAlchemy models
â”‚   â”‚   â”‚   â”œâ”€â”€ repository.py    # DB operations
â”‚   â”‚   â”‚   â””â”€â”€ migrations/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ price_service.py
â”‚   â”‚   â”‚   â””â”€â”€ meal_planner.py
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â”œâ”€â”€ routes.py
â”‚   â”‚       â””â”€â”€ schemas.py       # Pydantic models
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”‚   â””â”€â”€ scrape_tasks.py      # Scheduled scraping
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ next.config.js
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ seed_products.py         # Initial product list
â”‚   â””â”€â”€ run_scrape.py            # Manual scrape trigger
â””â”€â”€ docker-compose.yml
```

---

## Implementation Phases

### Phase 1: Price Fetcher POC (Week 1)
- [ ] Set up Python backend with FastAPI
- [ ] Implement Crawl4AI scraper for SPAR Vienna
- [ ] Create basic price repository (Supabase)
- [ ] Test: Fetch 10 products, store prices

### Phase 2: Multi-Store Scraping (Week 2)
- [ ] Add Tesco (UK) scraper
- [ ] Add BigBasket (India) scraper
- [ ] Implement scheduled scraping (cron)
- [ ] Build price comparison API

### Phase 3: Optimization Engine (Week 3)
- [ ] Implement linear programming optimizer
- [ ] Create meal plan generator
- [ ] Add nutritional constraints
- [ ] Test with real price data

### Phase 4: Frontend MVP (Week 4)
- [ ] Build Next.js frontend
- [ ] Budget input UI
- [ ] Meal plan display
- [ ] Shopping list with store links

### Phase 5: Launch (Week 5)
- [ ] Deploy to production
- [ ] Create demo video
- [ ] Post on Reddit, HN, Twitter
- [ ] Collect feedback

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Scraping blocked | Use Crawl4AI's proxy rotation, respect rate limits |
| Prices stale | Show "last updated" timestamp, refresh on demand |
| LLM costs high | Use local models (Ollama) for parsing, OpenAI only for complex tasks |
| Store layout changes | CSS selectors break â†’ fall back to LLM extraction |
| Legal concerns | Only scrape public data, cache locally, don't redistribute raw data |

---

## Success Metrics (MVP)

- [ ] Successfully scrape 50+ products from 4 stores
- [ ] Generate a valid â‚¬50/week meal plan for Vienna
- [ ] < 30 second response time for meal plan generation
- [ ] 5+ test users complete a full week using the plan
- [ ] At least 1 "would pay for this" response

